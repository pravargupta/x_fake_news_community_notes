{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPBDkeGTNZ1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93eaf180-e01f-451d-c724-f34521a0c486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              tweet_id                                             TWEETS  \\\n",
            "0  1783159712986382830  TV/Digital home of Bill O‚ÄôReilly, Jesse Kell...   \n",
            "1  1783171851818021181  WFLA News Channel 8 is a news leader in Tampa ...   \n",
            "2  1783154445682979015  TV/Digital home of Bill O‚ÄôReilly, Jesse Kell...   \n",
            "3  1377030478167937024  I am a United States Senator from the great st...   \n",
            "4  1536848327979016193  We bring you the power to compare offers and r...   \n",
            "\n",
            "                                     COMMUNITY NOTES favorite_count favorited  \\\n",
            "0  WHOA JOE!\\n\"I had a nurse named Pearl... She'd...          11057     FALSE   \n",
            "1  DRAMATIC VIDEO: Florida cop treated for overdo...            379     FALSE   \n",
            "2  WATCH - Bill Maher Trashes Woke Ideology\\n\"Fiv...             52     FALSE   \n",
            "3  My personal Twitter account ‚Äì @BasedMikeLee ...          13328     FALSE   \n",
            "4  Don't borrow from the bank.  Borrow from yours...            540     FALSE   \n",
            "\n",
            "  is_quote_status lang possibly_sensitive possibly_sensitive_editable  \\\n",
            "0           FALSE   en              FALSE                        TRUE   \n",
            "1           FALSE   en              FALSE                        TRUE   \n",
            "2           FALSE   en              FALSE                        TRUE   \n",
            "3           FALSE   en              FALSE                        TRUE   \n",
            "4           FALSE   en              FALSE                        TRUE   \n",
            "\n",
            "  quote_count  ... misleadingUnverifiedClaimAsFact misleadingSatire  \\\n",
            "0        1645  ...                               0                0   \n",
            "1         770  ...                               1                0   \n",
            "2           0  ...                               0                0   \n",
            "3         903  ...                               1                0   \n",
            "4          89  ...                               0                0   \n",
            "\n",
            "  notMisleadingOther notMisleadingFactuallyCorrect  \\\n",
            "0                  0                             0   \n",
            "1                  0                             0   \n",
            "2                  0                             0   \n",
            "3                  0                             0   \n",
            "4                  0                             0   \n",
            "\n",
            "  notMisleadingOutdatedButNotWhenWritten notMisleadingClearlySatire  \\\n",
            "0                                      0                          0   \n",
            "1                                      0                          0   \n",
            "2                                      0                          0   \n",
            "3                                      0                          0   \n",
            "4                                      0                          1   \n",
            "\n",
            "   notMisleadingPersonalOpinion trustworthySources  \\\n",
            "0                             0                  1   \n",
            "1                             0                  1   \n",
            "2                             0                  1   \n",
            "3                             0                  1   \n",
            "4                             1                  0   \n",
            "\n",
            "                                             summary  isMediaNote  \n",
            "0  The House failed to pass a border protection l...            0  \n",
            "1  The United States has 50 States     https://da...            0  \n",
            "2  TikTok only mentions “ban” and chooses to igno...            0  \n",
            "3  Forbes has a good rundown of the investigation...            0  \n",
            "4  They are expressing a personal opinion in a st...            0  \n",
            "\n",
            "[5 rows x 39 columns]\n",
            "Data types before scaling:\n",
            "favorite_count    object\n",
            "quote_count       object\n",
            "reply_count       object\n",
            "retweet_count     object\n",
            "dtype: object\n",
            "Checking for null values after conversion:\n",
            "favorite_count    48\n",
            "quote_count       22\n",
            "reply_count       15\n",
            "retweet_count     21\n",
            "dtype: int64\n",
            "Number of nodes: 1519\n",
            "Number of edges: 0\n",
            "Training and testing sets prepared.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/merged_file.csv\")  # Replace with the actual path to your dataset\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Step 2: Check the data types of the engagement features\n",
        "engagement_features = ['favorite_count', 'quote_count', 'reply_count', 'retweet_count']\n",
        "\n",
        "# Print data types for debugging\n",
        "print(\"Data types before scaling:\")\n",
        "print(df[engagement_features].dtypes)\n",
        "\n",
        "# Ensure engagement features are of numeric type\n",
        "for feature in engagement_features:\n",
        "    df[feature] = pd.to_numeric(df[feature], errors='coerce')  # Convert to numeric, forcing non-numeric to NaN\n",
        "\n",
        "# After conversion, check for null values\n",
        "print(\"Checking for null values after conversion:\")\n",
        "print(df[engagement_features].isnull().sum())\n",
        "\n",
        "# Drop any rows with NaN values in engagement features\n",
        "df.dropna(subset=engagement_features, inplace=True)\n",
        "\n",
        "# Scale only the numeric engagement features\n",
        "scaler = StandardScaler()\n",
        "df[engagement_features] = scaler.fit_transform(df[engagement_features])\n",
        "\n",
        "# Step 3: Create a graph using NetworkX\n",
        "G = nx.Graph()\n",
        "\n",
        "# Step 4: Add nodes to the graph using tweet_id\n",
        "for index, row in df.iterrows():\n",
        "    G.add_node(row['tweet_id'],  # Use tweet_id as the node\n",
        "               text=row['TWEETS'],\n",
        "               community_notes=row['COMMUNITY NOTES'],\n",
        "               engagement={\n",
        "                   'favorite_count': row['favorite_count'],\n",
        "                   'quote_count': row['quote_count'],\n",
        "                   'reply_count': row['reply_count'],\n",
        "                   'retweet_count': row['retweet_count']\n",
        "               },\n",
        "               misleading_features={\n",
        "                   'misleadingFactualError': row['misleadingFactualError'],\n",
        "                   'misleadingManipulatedMedia': row['misleadingManipulatedMedia'],\n",
        "                   'misleadingOutdatedInformation': row['misleadingOutdatedInformation'],\n",
        "                   'misleadingMissingImportantContext': row['misleadingMissingImportantContext'],\n",
        "                   'misleadingUnverifiedClaimAsFact': row['misleadingUnverifiedClaimAsFact'],\n",
        "                   'misleadingSatire': row['misleadingSatire']\n",
        "               },\n",
        "               not_misleading_features={\n",
        "                   'notMisleadingOther': row['notMisleadingOther'],\n",
        "                   'notMisleadingFactuallyCorrect': row['notMisleadingFactuallyCorrect'],\n",
        "               },\n",
        "               classification=row['classification'])  # Classification as the target variable\n",
        "\n",
        "# Step 5: Add edges based on retweets and replies\n",
        "# Assuming your dataset includes columns for retweet and reply IDs (modify as necessary)\n",
        "# for index, row in df.iterrows():\n",
        "    # Placeholder for retweet and reply logic\n",
        "    # Uncomment and modify based on your dataset structure\n",
        "    # if 'retweet_id' in row and row['retweet_id']:\n",
        "    #     G.add_edge(row['tweet_id'], row['retweet_id'], relation='retweet')\n",
        "\n",
        "    # if 'reply_to_id' in row and row['reply_to_id']:\n",
        "    #     G.add_edge(row['tweet_id'], row['reply_to_id'], relation='reply')\n",
        "\n",
        "# Print the graph summary\n",
        "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
        "print(f\"Number of edges: {G.number_of_edges()}\")\n",
        "\n",
        "# Step 6: Prepare the data for training/testing\n",
        "# Extract features and labels\n",
        "X = pd.DataFrame.from_records([G.nodes[node] for node in G.nodes()])\n",
        "y = X['classification']  # Assuming classification is the label for fake/misleading news\n",
        "X.drop('classification', axis=1, inplace=True)  # Remove the label from features\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: You can now use X_train, y_train for training your GNN model\n",
        "# Note: Implement the GNN model here, based on your requirements\n",
        "\n",
        "print(\"Training and testing sets prepared.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the logic for community notes agreement/disagreement\n",
        "def check_agreement(row):\n",
        "    # Simple keyword-based logic for agreement/disagreement\n",
        "    notes = row['COMMUNITY NOTES'].lower() if isinstance(row['COMMUNITY NOTES'], str) else ''\n",
        "    if \"misleading\" in notes or \"disagree\" in notes or \"false\" in notes:\n",
        "        return -1  # Disagreement\n",
        "    elif \"agree\" in notes or \"correct\" in notes or \"factually correct\" in notes:\n",
        "        return 1  # Agreement\n",
        "    else:\n",
        "        return 0  # Neutral/No relevant information\n",
        "\n",
        "# Step 2: Apply the logic to create a new column in the DataFrame\n",
        "df['community_agreement'] = df.apply(check_agreement, axis=1)\n",
        "\n",
        "# Step 3: Ensure that attributes are added to the graph correctly, including the new feature\n",
        "for index, row in df.iterrows():\n",
        "    G.add_node(row['tweet_id'],\n",
        "               favorite_count=row['favorite_count'],\n",
        "               quote_count=row['quote_count'],\n",
        "               reply_count=row['reply_count'],\n",
        "               retweet_count=row['retweet_count'],\n",
        "               classification=row['classification'],\n",
        "               community_agreement=row['community_agreement'])  # Add new feature\n",
        "\n",
        "# Step 4: Create Node Features Matrix including community_agreement\n",
        "node_features = []\n",
        "node_labels = []\n",
        "\n",
        "for node in G.nodes(data=True):\n",
        "    node_id = node[0]\n",
        "    features = node[1]  # Node data (attributes)\n",
        "\n",
        "    # Extract features safely\n",
        "    favorite_count = features.get('favorite_count', 0)\n",
        "    quote_count = features.get('quote_count', 0)\n",
        "    reply_count = features.get('reply_count', 0)\n",
        "    retweet_count = features.get('retweet_count', 0)\n",
        "    community_agreement = features.get('community_agreement', 0)  # New feature\n",
        "\n",
        "    # Append all features to node_features\n",
        "    node_features.append([\n",
        "        favorite_count,\n",
        "        quote_count,\n",
        "        reply_count,\n",
        "        retweet_count,\n",
        "        community_agreement  # Add community agreement/disagreement feature\n",
        "    ])\n",
        "\n",
        "    node_labels.append(features.get('classification', 0))  # Target label\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "node_features = np.array(node_features)\n",
        "node_labels = np.array(node_labels)\n",
        "\n",
        "# Continue with the rest of the code...\n"
      ],
      "metadata": {
        "id": "JucxlgxKVTHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# Assuming df is your DataFrame after all necessary preprocessing\n",
        "\n",
        "# Step 1: Define the logic for community notes agreement/disagreement\n",
        "def check_agreement(row):\n",
        "    # Simple keyword-based logic for agreement/disagreement\n",
        "    notes = row['COMMUNITY NOTES'].lower() if isinstance(row['COMMUNITY NOTES'], str) else ''\n",
        "    if \"misleading\" in notes or \"disagree\" in notes or \"false\" in notes:\n",
        "        return -1  # Disagreement\n",
        "    elif \"agree\" in notes or \"correct\" in notes or \"factually correct\" in notes:\n",
        "        return 1  # Agreement\n",
        "    else:\n",
        "        return 0  # Neutral/No relevant information\n",
        "\n",
        "# Step 2: Apply the logic to create a new column in the DataFrame\n",
        "df['community_agreement'] = df.apply(check_agreement, axis=1)\n",
        "\n",
        "# Step 3: Extract Node Features\n",
        "# Select relevant features for node representation, including the new community_agreement\n",
        "engagement_features = ['favorite_count', 'quote_count', 'reply_count', 'retweet_count', 'community_agreement']\n",
        "# Convert to numeric (handle any potential conversion issues)\n",
        "df[engagement_features] = df[engagement_features].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill missing values with 0 for scaling\n",
        "df[engagement_features] = df[engagement_features].fillna(0)\n",
        "\n",
        "# Create a node feature matrix\n",
        "node_features = df[engagement_features].values\n",
        "\n",
        "# Step 4: Extract Labels\n",
        "# Assuming the 'classification' column contains the label information\n",
        "labels = df['classification'].map({\n",
        "    'MISINFORMED_OR_POTENTIALLY_MISLEADING': 1,\n",
        "    'NOT_MISLEADING': 0\n",
        "})\n",
        "labels = labels.fillna(0).values  # Fill any missing labels with 0\n",
        "\n",
        "# Step 5: Extract Edges\n",
        "edges = []\n",
        "# Group by 'COMMUNITY NOTES' to find tweet IDs in the same note\n",
        "for community_note in df['COMMUNITY NOTES'].dropna().unique():\n",
        "    tweet_ids = df[df['COMMUNITY NOTES'] == community_note]['tweet_id'].values\n",
        "    if len(tweet_ids) > 1:\n",
        "        # Create edges for all unique pairs of tweet IDs in the same community note\n",
        "        for i in range(len(tweet_ids)):\n",
        "            for j in range(i + 1, len(tweet_ids)):\n",
        "                # Ensure no self-loops\n",
        "                if tweet_ids[i] != tweet_ids[j]:\n",
        "                    edges.append((tweet_ids[i], tweet_ids[j]))  # Only add non-self-loops\n",
        "\n",
        "# Convert edges to a NumPy array\n",
        "edge_index = np.array(edges).T  # Transpose for edge_index format\n",
        "\n",
        "# Now, you have updated edge_index\n",
        "print(\"Edge index shape after correction:\", edge_index.shape)\n",
        "# Display sample edges to verify\n",
        "print(\"Sample edges after correction:\", edges[:5])\n",
        "\n",
        "# Now, you have node_features, labels, and edge_index prepared\n",
        "\n",
        "# Display the shapes of the created data components\n",
        "print(\"Node features shape:\", node_features.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "print(\"Edge index shape:\", edge_index.shape)\n",
        "\n",
        "# To verify edges created, you can print a sample\n",
        "print(\"Sample edges:\", edges[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iocNjC09WZgx",
        "outputId": "0d8829e7-fef0-4e08-8a75-78c4d0aa5fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge index shape after correction: (2, 2336)\n",
            "Sample edges after correction: [(1742206236420927754, 1816439860388848082), (1742206236420927754, 1638007824830849027), (1742206236420927754, 1376258278636855305), (1816439860388848082, 1638007824830849027), (1816439860388848082, 1376258278636855305)]\n",
            "Node features shape: (1534, 5)\n",
            "Labels shape: (1534,)\n",
            "Edge index shape: (2, 2336)\n",
            "Sample edges: [(1742206236420927754, 1816439860388848082), (1742206236420927754, 1638007824830849027), (1742206236420927754, 1376258278636855305), (1816439860388848082, 1638007824830849027), (1816439860388848082, 1376258278636855305)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create edge index based on train indices\n",
        "train_mask = np.isin(edge_index[0], train_indices) & np.isin(edge_index[1], train_indices)\n",
        "test_mask = np.isin(edge_index[0], test_indices) & np.isin(edge_index[1], test_indices)\n",
        "\n",
        "# Filter the edge index based on the masks\n",
        "train_edge_index = edge_index[:, train_mask]\n",
        "test_edge_index = edge_index[:, test_mask]\n"
      ],
      "metadata": {
        "id": "M5wPSC3pXL_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p10LoSTAXd7m",
        "outputId": "ab2e26bf-9bfc-4e73-bb48-ff6776c6e6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.8)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Ensure node_features and labels are still DataFrames/Series\n",
        "node_features = pd.DataFrame(node_features)  # Ensure it's a DataFrame\n",
        "labels = pd.Series(labels)  # Ensure labels are a Series\n",
        "\n",
        "# Perform the train-test split while preserving indices\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    node_features,\n",
        "    labels,\n",
        "    test_size=0.2,  # 20% for testing\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Get the indices for training and testing sets\n",
        "train_indices = X_train.index.tolist()\n",
        "test_indices = X_test.index.tolist()\n",
        "\n",
        "# Create edge index based on train indices\n",
        "train_mask = np.isin(edge_index[0], train_indices) & np.isin(edge_index[1], train_indices)\n",
        "test_mask = np.isin(edge_index[0], test_indices) & np.isin(edge_index[1], test_indices)\n",
        "\n",
        "# Filter the edge index based on the masks\n",
        "train_edge_index = edge_index[:, train_mask]\n",
        "test_edge_index = edge_index[:, test_mask]\n",
        "\n",
        "# Convert the edge index to torch tensors\n",
        "train_edge_index = torch.tensor(train_edge_index, dtype=torch.long)\n",
        "test_edge_index = torch.tensor(test_edge_index, dtype=torch.long)\n",
        "\n",
        "# Convert node features and labels for training to tensors\n",
        "x_train = torch.tensor(X_train.values, dtype=torch.float)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float).view(-1, 1)  # Ensure y has the right shape for regression\n",
        "\n",
        "# Create Data object for training\n",
        "train_data = Data(x=x_train, edge_index=train_edge_index, y=y_train)\n",
        "\n",
        "# Convert node features and labels for testing to tensors\n",
        "x_test = torch.tensor(X_test.values, dtype=torch.float)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.float).view(-1, 1)  # Ensure y has the right shape for regression\n",
        "\n",
        "# Create Data object for testing\n",
        "test_data = Data(x=x_test, edge_index=test_edge_index, y=y_test)\n",
        "\n",
        "# Print the final Data objects\n",
        "print(\"Train Data:\", train_data)\n",
        "print(\"Test Data:\", test_data)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader([test_data], batch_size=32, shuffle=False)\n",
        "\n",
        "# Checking the DataLoader\n",
        "for batch in train_loader:\n",
        "    print(\"Train Batch:\", batch)\n",
        "\n",
        "for batch in test_loader:\n",
        "    print(\"Test Batch:\", batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE2Dx8NIXaEj",
        "outputId": "8a0b4386-5b52-46e2-c1dc-6a41b84df91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data: Data(x=[1227, 5], edge_index=[2, 0], y=[1227, 1])\n",
            "Test Data: Data(x=[307, 5], edge_index=[2, 0], y=[307, 1])\n",
            "Train Batch: DataBatch(x=[1227, 5], edge_index=[2, 0], y=[1227, 1], batch=[1227], ptr=[2])\n",
            "Test Batch: DataBatch(x=[307, 5], edge_index=[2, 0], y=[307, 1], batch=[307], ptr=[2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original edge index shape:\", edge_index.shape)\n",
        "print(\"Number of unique nodes in edge_index:\", np.unique(edge_index).shape[0])\n",
        "print(\"Train indices unique nodes:\", np.unique(train_indices).shape[0])\n",
        "print(\"Test indices unique nodes:\", np.unique(test_indices).shape[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KFREmHgYJlV",
        "outputId": "2bda855e-b1b2-419d-bcf7-66db66c9a2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original edge index shape: (2, 2336)\n",
            "Number of unique nodes in edge_index: 223\n",
            "Train indices unique nodes: 1227\n",
            "Test indices unique nodes: 307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Step 1: Define the GNN model\n",
        "class GNNModel(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 16)\n",
        "        self.conv2 = GCNConv(16, 2)  # Assuming binary classification (0 or 1)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Step 2: Initialize model, optimizer, and loss function\n",
        "num_features = node_features.shape[1]  # Update based on your node feature shape\n",
        "model = GNNModel(num_features)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = F.cross_entropy  # Assuming binary classification\n",
        "\n",
        "# Step 3: Convert edge_index to tensor\n",
        "edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)  # Directly convert to tensor\n",
        "\n",
        "# Step 4: Create mapping from original node IDs to their indices\n",
        "original_node_ids = torch.unique(edge_index_tensor)\n",
        "node_id_to_index = {node_id.item(): index for index, node_id in enumerate(original_node_ids)}\n",
        "\n",
        "# Step 5: Update edge_index to use the new indices\n",
        "updated_edge_index = []\n",
        "for edge in edge_index_tensor.t().tolist():\n",
        "    updated_edge_index.append([node_id_to_index[edge[0]], node_id_to_index[edge[1]]])\n",
        "updated_edge_index = torch.tensor(updated_edge_index, dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Step 6: Create data loaders\n",
        "train_data = train_data.__class__(x=train_data.x, edge_index=updated_edge_index, y=train_data.y.long())\n",
        "test_data = test_data.__class__(x=test_data.x, edge_index=updated_edge_index, y=test_data.y.long())\n",
        "\n",
        "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader([test_data], batch_size=32, shuffle=False)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 7: Train the model\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0  # Initialize training loss\n",
        "\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "\n",
        "        # Calculate loss, squeeze data.y to get rid of the singleton dimension\n",
        "        loss = loss_fn(out, data.y.squeeze().long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()  # Accumulate training loss\n",
        "\n",
        "    # Average training loss for the epoch\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Step 8: Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    test_loss = 0  # Initialize test loss\n",
        "    y_true_test = []\n",
        "    y_pred_test = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            out = model(data.x, data.edge_index)\n",
        "            # Calculate test loss\n",
        "            loss = loss_fn(out, data.y.squeeze().long())\n",
        "            test_loss += loss.item()  # Accumulate test loss\n",
        "\n",
        "            # Gather predictions and true labels\n",
        "            y_true_test.extend(data.y.squeeze().tolist())\n",
        "            y_pred_test.extend(out.argmax(dim=1).tolist())\n",
        "\n",
        "    # Average test loss for the epoch\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_acc = accuracy_score(y_true_test, y_pred_test)\n",
        "    test_f1 = f1_score(y_true_test, y_pred_test, average='weighted')\n",
        "\n",
        "    # Display the metrics\n",
        "    print(f'Epoch: {epoch + 1} | TrainLoss: {train_loss:.2f} | TestLoss: {test_loss:.2f} | TestAcc: {test_acc:.2f} | TestF1: {test_f1:.2f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 8: Evaluate the model on the test set\n",
        "y_true_test = []\n",
        "y_pred_test = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        out = model(data.x, data.edge_index)\n",
        "        y_true_test.extend(data.y.tolist())\n",
        "        y_pred_test.extend(out.argmax(dim=1).tolist())\n",
        "\n",
        "# Final F1 score on the test set\n",
        "f1_test = f1_score(y_true_test, y_pred_test, average='weighted')\n",
        "print(f'Final Test F1 Score: {f1_test:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EYuxiTkbCj_",
        "outputId": "95d83da9-b59c-46fc-c3cd-b9a46d5ae02d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | TrainLoss: 0.75 | TestLoss: 0.62 | TestAcc: 0.54 | TestF1: 0.59\n",
            "Epoch: 2 | TrainLoss: 0.70 | TestLoss: 0.59 | TestAcc: 0.80 | TestF1: 0.73\n",
            "Epoch: 3 | TrainLoss: 0.67 | TestLoss: 0.58 | TestAcc: 0.81 | TestF1: 0.73\n",
            "Epoch: 4 | TrainLoss: 0.65 | TestLoss: 0.56 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 5 | TrainLoss: 0.62 | TestLoss: 0.55 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 6 | TrainLoss: 0.60 | TestLoss: 0.54 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 7 | TrainLoss: 0.58 | TestLoss: 0.53 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 8 | TrainLoss: 0.57 | TestLoss: 0.52 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 9 | TrainLoss: 0.56 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 10 | TrainLoss: 0.54 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 11 | TrainLoss: 0.55 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 12 | TrainLoss: 0.54 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 13 | TrainLoss: 0.53 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 14 | TrainLoss: 0.52 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 15 | TrainLoss: 0.53 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 16 | TrainLoss: 0.51 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 17 | TrainLoss: 0.52 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 18 | TrainLoss: 0.52 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 19 | TrainLoss: 0.53 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 20 | TrainLoss: 0.53 | TestLoss: 0.52 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 21 | TrainLoss: 0.52 | TestLoss: 0.52 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 22 | TrainLoss: 0.53 | TestLoss: 0.52 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 23 | TrainLoss: 0.53 | TestLoss: 0.52 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 24 | TrainLoss: 0.53 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 25 | TrainLoss: 0.53 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 26 | TrainLoss: 0.52 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 27 | TrainLoss: 0.51 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 28 | TrainLoss: 0.50 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 29 | TrainLoss: 0.51 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 30 | TrainLoss: 0.51 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 31 | TrainLoss: 0.51 | TestLoss: 0.51 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 32 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 33 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 34 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 35 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 36 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 37 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 38 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 39 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 40 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 41 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 42 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 43 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 44 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 45 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 46 | TrainLoss: 0.49 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 47 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 48 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 49 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 50 | TrainLoss: 0.51 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 51 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 52 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 53 | TrainLoss: 0.49 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 54 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 55 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 56 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 57 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 58 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 59 | TrainLoss: 0.50 | TestLoss: 0.50 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 60 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 61 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 62 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 63 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 64 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 65 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 66 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 67 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 68 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 69 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 70 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 71 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 72 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 73 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 74 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 75 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 76 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 77 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 78 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 79 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 80 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 81 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 82 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 83 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 84 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 85 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 86 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 87 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 88 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 89 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 90 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 91 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 92 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 93 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 94 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 95 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 96 | TrainLoss: 0.50 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 97 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 98 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 99 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Epoch: 100 | TrainLoss: 0.49 | TestLoss: 0.49 | TestAcc: 0.81 | TestF1: 0.72\n",
            "Final Test F1 Score: 0.7219\n"
          ]
        }
      ]
    }
  ]
}